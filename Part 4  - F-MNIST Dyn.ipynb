{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packeges\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device. Device is either cuda:0 or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: F_MNIST_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: F_MNIST_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# define a trandform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# download the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data', train=True, download=True, transform=transform)\n",
    "\n",
    "# download the testing data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devide the training set into a training and a validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "shuffle_dataset = True\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "trainset_size = len(trainset)\n",
    "indices = list(range(trainset_size))\n",
    "split = int(np.floor(validation_split * trainset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "validation_loader = DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "helper.imshow(images[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possible ways to build a network: 1. a static one 2. a dynamic one. \n",
    "<br>\n",
    "\n",
    "The dynamic one allows us to configure as many hidden layers as we want just by modifying a patrameter list of the hidden layers. This is done with the help of [`nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#modulelist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_layer_size, output_layer_size, hidden_layers, drop_p = 0.5):\n",
    "        ''' build a feedforward network with arbitraty many hidden layers\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_layer: integer, size of the input\n",
    "            hidden_layers: list of integers, amount and size of the hidden layers\n",
    "            output_layer: integer, size of the output layer\n",
    "            drop_p: float between 0 and 1, dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # add the first layer input to hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_layer_size, hidden_layers[0])])\n",
    "        \n",
    "        # add a viariable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_layer_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = drop_p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        # Forward through each layer in the hidden_layers, with ReLU activation and dropouts\n",
    "        for linear in self.hidden_layers:\n",
    "            x = F.relu(linear(x))\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fcl1): Linear(in_features=784, out_features=800, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fcl2): Linear(in_features=800, out_features=800, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fcl3): Linear(in_features=800, out_features=600, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fcl4): Linear(in_features=600, out_features=600, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (fcl5): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (relu5): ReLU()\n",
       "  (fcl6): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (fcl7): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (relu7): ReLU()\n",
       "  (fcl8): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu8): ReLU()\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = 784\n",
    "hidden_layers = [800, 600, 400, 200]\n",
    "output_layer = 10\n",
    "\n",
    "netmodel = nn.Sequential(OrderedDict([\n",
    "    ('fcl1', nn.Linear(input_layer, hidden_layers[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fcl2', nn.Linear(hidden_layers[0], hidden_layers[0])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fcl3', nn.Linear(hidden_layers[0], hidden_layers[1])),\n",
    "    ('relu3', nn.ReLU()),\n",
    "    ('fcl4', nn.Linear(hidden_layers[1], hidden_layers[1])),\n",
    "    ('relu4', nn.ReLU()),\n",
    "    ('fcl5', nn.Linear(hidden_layers[1], hidden_layers[2])),\n",
    "    ('relu5', nn.ReLU()),\n",
    "    ('fcl6', nn.Linear(hidden_layers[2], hidden_layers[2])),\n",
    "    ('relu6', nn.ReLU()),\n",
    "    ('fcl7', nn.Linear(hidden_layers[2], hidden_layers[3])),\n",
    "    ('relu7', nn.ReLU()),\n",
    "    ('fcl8', nn.Linear(hidden_layers[3], hidden_layers[3])),\n",
    "    ('relu8', nn.ReLU()),\n",
    "    ('output', nn.Linear(hidden_layers[3], output_layer))\n",
    "]))\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the network - dynamic or static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnDynamic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nnDynamic == False:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(netmodel.parameters(), lr=0.001)\n",
    "    optimizer = optim.Adam(netmodel.parameters(), lr=0.0003)\n",
    "    #optimizer = optim.SGD(netmodel.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (2): Linear(in_features=800, out_features=600, bias=True)\n",
       "    (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (4): Linear(in_features=600, out_features=400, bias=True)\n",
       "    (5): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (6): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (7): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.0)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if nnDynamic == True:\n",
    "    input_layer_size = 784\n",
    "    output_layer_size = 10\n",
    "    #hidden_layers_list = [512, 256, 128, 64]\n",
    "    hidden_layers_list = [800, 800, 600, 600, 400, 400, 200, 200]\n",
    "    dropout_p = 0.0\n",
    "    netmodel = Network(input_layer_size, output_layer_size, hidden_layers_list, dropout_p)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(netmodel.parameters(), lr=0.003)\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12652838912\n",
      "94741504\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_properties(device).total_memory)\n",
    "print(torch.cuda.max_memory_allocated(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Epoch: 1/1...  Loss: 1.8497 Validation Loss: 1.558..  Validation Accuracy: 0.234\n",
      "Epoch: 1/1...  Loss: 1.4789 Validation Loss: 1.341..  Validation Accuracy: 0.471\n",
      "Epoch: 1/1...  Loss: 1.2313 Validation Loss: 1.141..  Validation Accuracy: 0.494\n",
      "Epoch: 1/1...  Loss: 1.1144 Validation Loss: 0.974..  Validation Accuracy: 0.578\n",
      "Epoch: 1/1...  Loss: 0.9757 Validation Loss: 0.886..  Validation Accuracy: 0.600\n",
      "Epoch: 1/1...  Loss: 0.8385 Validation Loss: 0.832..  Validation Accuracy: 0.667\n",
      "Epoch: 1/1...  Loss: 0.7762 Validation Loss: 0.918..  Validation Accuracy: 0.615\n",
      "Epoch: 1/1...  Loss: 0.8279 Validation Loss: 0.862..  Validation Accuracy: 0.629\n",
      "Epoch: 1/1...  Loss: 0.8248 Validation Loss: 0.748..  Validation Accuracy: 0.726\n",
      "Epoch: 1/1...  Loss: 0.7050 Validation Loss: 0.654..  Validation Accuracy: 0.759\n",
      "Epoch: 1/1...  Loss: 0.7292 Validation Loss: 0.655..  Validation Accuracy: 0.756\n",
      "Epoch: 1/1...  Loss: 0.7266 Validation Loss: 0.704..  Validation Accuracy: 0.750\n",
      "Epoch: 1/1...  Loss: 0.8095 Validation Loss: 0.839..  Validation Accuracy: 0.660\n",
      "Epoch: 1/1...  Loss: 0.7120 Validation Loss: 0.708..  Validation Accuracy: 0.758\n",
      "Epoch: 1/1...  Loss: 0.6956 Validation Loss: 0.717..  Validation Accuracy: 0.712\n",
      "Epoch: 1/1...  Loss: 0.6967 Validation Loss: 0.647..  Validation Accuracy: 0.777\n",
      "Epoch: 1/1...  Loss: 0.6213 Validation Loss: 0.642..  Validation Accuracy: 0.784\n",
      "Epoch: 1/1...  Loss: 0.6766 Validation Loss: 0.656..  Validation Accuracy: 0.754\n",
      "Time for training and validation : 0 minutes and 27.320 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print(type(running_loss))\n",
    "print_every = 40\n",
    "\n",
    "netmodel.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        \n",
    "        # flatten the imiga into a 784 element vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = netmodel.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            netmodel.eval()\n",
    "        \n",
    "            accuracy = 0\n",
    "            valid_loss = 0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(validation_loader):\n",
    "                \n",
    "                \n",
    "                # flatten the imiga into a 784 element vector\n",
    "                images.resize_(images.size()[0], 784)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = netmodel.forward(images)\n",
    "                valid_loss += criterion(output, labels).item()\n",
    "                \n",
    "                if nnDynamic == True:\n",
    "                    ps = torch.exp(output)\n",
    "                else:\n",
    "                    ps = F.softmax(output, dim=1)\n",
    "                \n",
    "                \n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                \n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Validation Loss: {:.3f}.. \".format(valid_loss/len(validation_loader)),\n",
    "                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(validation_loader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            netmodel.train()\n",
    "            \n",
    "print(\"Time for training and validation : {:.0f} minutes and {:.3f} seconds\".format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12652838912\n",
      "117437440\n",
      "113992704\n",
      "117437440\n",
      "113992704\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_properties(device).total_memory)\n",
    "print(torch.cuda.max_memory_allocated(device))\n",
    "print(torch.cuda.memory_allocated(device))\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.max_memory_allocated(device))\n",
    "print(torch.cuda.memory_allocated(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.793\n"
     ]
    }
   ],
   "source": [
    "netmodel.eval()\n",
    "\n",
    "netmodel.to(device)\n",
    "        \n",
    "accuracy = 0\n",
    "test_loss = 0\n",
    "\n",
    "for ii, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "    # flatten the imiga into a 784 element vector\n",
    "    images.resize_(images.size()[0], 784)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = netmodel.forward(images)\n",
    "\n",
    "    if nnDynamic == True:\n",
    "        ps = torch.exp(output)\n",
    "    else:\n",
    "        ps = F.softmax(output, dim=1)\n",
    "    \n",
    "\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out your network!\n",
    "netmodel.to(device)\n",
    "\n",
    "netmodel.eval()\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    output = netmodel.forward(img)\n",
    "\n",
    "if nnDynamic == True:\n",
    "    ps = torch.exp(output)\n",
    "else:\n",
    "    ps = F.softmax(logits, dim=1)\n",
    "\n",
    "#print(ps.size())\n",
    "#print(ps.max(1)[1])\n",
    "\n",
    "#print(ps.device, img.device)\n",
    "if device != 'cpu':\n",
    "    ps = ps.cpu()\n",
    "    img = img.cpu()\n",
    "#print(ps.device, img.device)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
