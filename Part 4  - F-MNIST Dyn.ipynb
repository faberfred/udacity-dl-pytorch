{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packeges\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device. Device is either cuda:0 or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: F_MNIST_data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: F_MNIST_data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# define a trandform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# download the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data', train=True, download=True, transform=transform)\n",
    "\n",
    "# download the testing data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devide the training set into a training and a validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "shuffle_dataset = True\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "trainset_size = len(trainset)\n",
    "indices = list(range(trainset_size))\n",
    "split = int(np.floor(validation_split * trainset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "validation_loader = DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "helper.imshow(images[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possible ways to build a network: 1. a static one 2. a dynamic one. \n",
    "<br>\n",
    "\n",
    "The dynamic one allows us to configure as many hidden layers as we want just by modifying a parameter list of the hidden layers. This is done with the help of [`nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#modulelist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_layer_size, output_layer_size, hidden_layers, drop_p = 0.5):\n",
    "        ''' build a feedforward network with arbitraty many hidden layers\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_layer: integer, size of the input\n",
    "            hidden_layers: list of integers, amount and size of the hidden layers\n",
    "            output_layer: integer, size of the output layer\n",
    "            drop_p: float between 0 and 1, dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # add the first layer input to hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_layer_size, hidden_layers[0])])\n",
    "        \n",
    "        # add a viariable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_layer_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = drop_p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        # Forward through each layer in the hidden_layers, with ReLU activation and dropouts\n",
    "        for linear in self.hidden_layers:\n",
    "            x = F.relu(linear(x))\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fcl1): Linear(in_features=784, out_features=800, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fcl2): Linear(in_features=800, out_features=800, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fcl3): Linear(in_features=800, out_features=600, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fcl4): Linear(in_features=600, out_features=600, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (fcl5): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (relu5): ReLU()\n",
       "  (fcl6): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (fcl7): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (relu7): ReLU()\n",
       "  (fcl8): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu8): ReLU()\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = 784\n",
    "hidden_layers = [800, 600, 400, 200]\n",
    "output_layer = 10\n",
    "\n",
    "netmodel = nn.Sequential(OrderedDict([\n",
    "    ('fcl1', nn.Linear(input_layer, hidden_layers[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fcl2', nn.Linear(hidden_layers[0], hidden_layers[0])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fcl3', nn.Linear(hidden_layers[0], hidden_layers[1])),\n",
    "    ('relu3', nn.ReLU()),\n",
    "    ('fcl4', nn.Linear(hidden_layers[1], hidden_layers[1])),\n",
    "    ('relu4', nn.ReLU()),\n",
    "    ('fcl5', nn.Linear(hidden_layers[1], hidden_layers[2])),\n",
    "    ('relu5', nn.ReLU()),\n",
    "    ('fcl6', nn.Linear(hidden_layers[2], hidden_layers[2])),\n",
    "    ('relu6', nn.ReLU()),\n",
    "    ('fcl7', nn.Linear(hidden_layers[2], hidden_layers[3])),\n",
    "    ('relu7', nn.ReLU()),\n",
    "    ('fcl8', nn.Linear(hidden_layers[3], hidden_layers[3])),\n",
    "    ('relu8', nn.ReLU()),\n",
    "    ('output', nn.Linear(hidden_layers[3], output_layer))\n",
    "]))\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the network - dynamic or static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnDynamic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nnDynamic == False:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(netmodel.parameters(), lr=0.001)\n",
    "    optimizer = optim.Adam(netmodel.parameters(), lr=0.0003)\n",
    "    #optimizer = optim.SGD(netmodel.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (2): Linear(in_features=800, out_features=600, bias=True)\n",
       "    (3): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (4): Linear(in_features=600, out_features=400, bias=True)\n",
       "    (5): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (6): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (7): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if nnDynamic == True:\n",
    "    input_layer_size = 784\n",
    "    output_layer_size = 10\n",
    "    #hidden_layers_list = [512, 256, 128, 64]\n",
    "    hidden_layers_list = [800, 800, 600, 600, 400, 400, 200, 200]\n",
    "    dropout_p = 0.0\n",
    "    netmodel = Network(input_layer_size, output_layer_size, hidden_layers_list, dropout_p)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(netmodel.parameters(), lr=0.003)\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12652838912\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_properties(device).total_memory)\n",
    "print(torch.cuda.max_memory_allocated(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Epoch: 1/4...  Loss: 2.1418 Validation Loss: 1.753..  Validation Accuracy: 0.193\n",
      "Epoch: 1/4...  Loss: 1.6854 Validation Loss: 1.605..  Validation Accuracy: 0.300\n",
      "Epoch: 1/4...  Loss: 1.5149 Validation Loss: 1.465..  Validation Accuracy: 0.287\n",
      "Epoch: 1/4...  Loss: 1.4511 Validation Loss: 1.297..  Validation Accuracy: 0.405\n",
      "Epoch: 1/4...  Loss: 1.2591 Validation Loss: 1.008..  Validation Accuracy: 0.497\n",
      "Epoch: 1/4...  Loss: 1.0644 Validation Loss: 0.998..  Validation Accuracy: 0.551\n",
      "Epoch: 1/4...  Loss: 0.9648 Validation Loss: 1.018..  Validation Accuracy: 0.545\n",
      "Epoch: 1/4...  Loss: 0.8738 Validation Loss: 0.907..  Validation Accuracy: 0.658\n",
      "Epoch: 1/4...  Loss: 0.8749 Validation Loss: 1.014..  Validation Accuracy: 0.586\n",
      "Epoch: 1/4...  Loss: 0.8661 Validation Loss: 0.818..  Validation Accuracy: 0.675\n",
      "Epoch: 1/4...  Loss: 0.8009 Validation Loss: 0.755..  Validation Accuracy: 0.721\n",
      "Epoch: 1/4...  Loss: 0.7416 Validation Loss: 0.826..  Validation Accuracy: 0.670\n",
      "Epoch: 1/4...  Loss: 0.7899 Validation Loss: 0.731..  Validation Accuracy: 0.733\n",
      "Epoch: 1/4...  Loss: 0.7388 Validation Loss: 0.700..  Validation Accuracy: 0.743\n",
      "Epoch: 1/4...  Loss: 0.7274 Validation Loss: 0.702..  Validation Accuracy: 0.745\n",
      "Epoch: 1/4...  Loss: 0.6922 Validation Loss: 0.752..  Validation Accuracy: 0.725\n",
      "Epoch: 1/4...  Loss: 0.6789 Validation Loss: 0.661..  Validation Accuracy: 0.758\n",
      "Epoch: 1/4...  Loss: 0.6606 Validation Loss: 0.638..  Validation Accuracy: 0.758\n",
      "Epoch: 2/4...  Loss: 0.1851 Validation Loss: 0.679..  Validation Accuracy: 0.751\n",
      "Epoch: 2/4...  Loss: 0.5913 Validation Loss: 0.611..  Validation Accuracy: 0.780\n",
      "Epoch: 2/4...  Loss: 0.6150 Validation Loss: 0.617..  Validation Accuracy: 0.778\n",
      "Epoch: 2/4...  Loss: 0.5663 Validation Loss: 0.602..  Validation Accuracy: 0.769\n",
      "Epoch: 2/4...  Loss: 0.5714 Validation Loss: 0.616..  Validation Accuracy: 0.779\n",
      "Epoch: 2/4...  Loss: 0.5876 Validation Loss: 0.597..  Validation Accuracy: 0.775\n",
      "Epoch: 2/4...  Loss: 0.5946 Validation Loss: 0.642..  Validation Accuracy: 0.776\n",
      "Epoch: 2/4...  Loss: 0.6011 Validation Loss: 0.563..  Validation Accuracy: 0.793\n",
      "Epoch: 2/4...  Loss: 0.6238 Validation Loss: 0.640..  Validation Accuracy: 0.800\n",
      "Epoch: 2/4...  Loss: 0.6278 Validation Loss: 0.632..  Validation Accuracy: 0.766\n",
      "Epoch: 2/4...  Loss: 0.5725 Validation Loss: 0.667..  Validation Accuracy: 0.770\n",
      "Epoch: 2/4...  Loss: 0.6304 Validation Loss: 0.578..  Validation Accuracy: 0.792\n",
      "Epoch: 2/4...  Loss: 0.6366 Validation Loss: 0.690..  Validation Accuracy: 0.767\n",
      "Epoch: 2/4...  Loss: 0.5690 Validation Loss: 0.562..  Validation Accuracy: 0.807\n",
      "Epoch: 2/4...  Loss: 0.5300 Validation Loss: 0.544..  Validation Accuracy: 0.797\n",
      "Epoch: 2/4...  Loss: 0.5302 Validation Loss: 0.528..  Validation Accuracy: 0.810\n",
      "Epoch: 2/4...  Loss: 0.5669 Validation Loss: 0.562..  Validation Accuracy: 0.812\n",
      "Epoch: 2/4...  Loss: 0.5803 Validation Loss: 0.559..  Validation Accuracy: 0.810\n",
      "Epoch: 2/4...  Loss: 0.5517 Validation Loss: 0.602..  Validation Accuracy: 0.795\n",
      "Epoch: 3/4...  Loss: 0.2698 Validation Loss: 0.587..  Validation Accuracy: 0.804\n",
      "Epoch: 3/4...  Loss: 0.5512 Validation Loss: 0.570..  Validation Accuracy: 0.810\n",
      "Epoch: 3/4...  Loss: 0.5117 Validation Loss: 0.531..  Validation Accuracy: 0.816\n",
      "Epoch: 3/4...  Loss: 0.5482 Validation Loss: 0.531..  Validation Accuracy: 0.811\n",
      "Epoch: 3/4...  Loss: 0.5913 Validation Loss: 0.693..  Validation Accuracy: 0.746\n",
      "Epoch: 3/4...  Loss: 0.5951 Validation Loss: 0.538..  Validation Accuracy: 0.815\n",
      "Epoch: 3/4...  Loss: 0.5198 Validation Loss: 0.601..  Validation Accuracy: 0.807\n",
      "Epoch: 3/4...  Loss: 0.6558 Validation Loss: 0.544..  Validation Accuracy: 0.805\n",
      "Epoch: 3/4...  Loss: 0.5253 Validation Loss: 0.529..  Validation Accuracy: 0.812\n",
      "Epoch: 3/4...  Loss: 0.5319 Validation Loss: 0.502..  Validation Accuracy: 0.826\n",
      "Epoch: 3/4...  Loss: 0.4968 Validation Loss: 0.515..  Validation Accuracy: 0.832\n",
      "Epoch: 3/4...  Loss: 0.4697 Validation Loss: 0.506..  Validation Accuracy: 0.823\n",
      "Epoch: 3/4...  Loss: 0.5194 Validation Loss: 0.487..  Validation Accuracy: 0.836\n",
      "Epoch: 3/4...  Loss: 0.5251 Validation Loss: 0.544..  Validation Accuracy: 0.826\n",
      "Epoch: 3/4...  Loss: 0.5554 Validation Loss: 0.509..  Validation Accuracy: 0.823\n",
      "Epoch: 3/4...  Loss: 0.4929 Validation Loss: 0.490..  Validation Accuracy: 0.829\n",
      "Epoch: 3/4...  Loss: 0.5108 Validation Loss: 0.540..  Validation Accuracy: 0.805\n",
      "Epoch: 3/4...  Loss: 0.5035 Validation Loss: 0.529..  Validation Accuracy: 0.811\n",
      "Epoch: 3/4...  Loss: 0.5355 Validation Loss: 0.512..  Validation Accuracy: 0.831\n",
      "Epoch: 4/4...  Loss: 0.4165 Validation Loss: 0.619..  Validation Accuracy: 0.824\n",
      "Epoch: 4/4...  Loss: 0.5336 Validation Loss: 0.490..  Validation Accuracy: 0.824\n",
      "Epoch: 4/4...  Loss: 0.4482 Validation Loss: 0.495..  Validation Accuracy: 0.835\n",
      "Epoch: 4/4...  Loss: 0.5320 Validation Loss: 0.570..  Validation Accuracy: 0.825\n",
      "Epoch: 4/4...  Loss: 0.5189 Validation Loss: 0.577..  Validation Accuracy: 0.794\n",
      "Epoch: 4/4...  Loss: 0.5308 Validation Loss: 0.543..  Validation Accuracy: 0.822\n",
      "Epoch: 4/4...  Loss: 0.4790 Validation Loss: 0.467..  Validation Accuracy: 0.840\n",
      "Epoch: 4/4...  Loss: 0.4334 Validation Loss: 0.493..  Validation Accuracy: 0.833\n",
      "Epoch: 4/4...  Loss: 0.4607 Validation Loss: 0.477..  Validation Accuracy: 0.836\n",
      "Epoch: 4/4...  Loss: 0.4620 Validation Loss: 0.480..  Validation Accuracy: 0.836\n",
      "Epoch: 4/4...  Loss: 0.4750 Validation Loss: 0.511..  Validation Accuracy: 0.829\n",
      "Epoch: 4/4...  Loss: 0.5017 Validation Loss: 0.460..  Validation Accuracy: 0.843\n",
      "Epoch: 4/4...  Loss: 0.4657 Validation Loss: 0.529..  Validation Accuracy: 0.829\n",
      "Epoch: 4/4...  Loss: 0.4673 Validation Loss: 0.491..  Validation Accuracy: 0.833\n",
      "Epoch: 4/4...  Loss: 0.4560 Validation Loss: 0.482..  Validation Accuracy: 0.839\n",
      "Epoch: 4/4...  Loss: 0.4704 Validation Loss: 0.453..  Validation Accuracy: 0.842\n",
      "Epoch: 4/4...  Loss: 0.4608 Validation Loss: 0.482..  Validation Accuracy: 0.827\n",
      "Epoch: 4/4...  Loss: 0.4618 Validation Loss: 0.521..  Validation Accuracy: 0.821\n",
      "Epoch: 4/4...  Loss: 0.5654 Validation Loss: 0.520..  Validation Accuracy: 0.823\n",
      "Time for training and validation : 2 minutes and 24.001 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print(type(running_loss))\n",
    "print_every = 40\n",
    "\n",
    "netmodel.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        \n",
    "        # flatten the imiga into a 784 element vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = netmodel.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            netmodel.eval()\n",
    "        \n",
    "            accuracy = 0\n",
    "            valid_loss = 0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(validation_loader):\n",
    "                \n",
    "                \n",
    "                # flatten the imiga into a 784 element vector\n",
    "                images.resize_(images.size()[0], 784)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = netmodel.forward(images)\n",
    "                valid_loss += criterion(output, labels).item()\n",
    "                \n",
    "                if nnDynamic == True:\n",
    "                    ps = torch.exp(output)\n",
    "                else:\n",
    "                    ps = F.softmax(output, dim=1)\n",
    "                \n",
    "                \n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                \n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Validation Loss: {:.3f}.. \".format(valid_loss/len(validation_loader)),\n",
    "                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(validation_loader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            netmodel.train()\n",
    "            \n",
    "print(\"Time for training and validation : {:.0f} minutes and {:.3f} seconds\".format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12652838912\n",
      "48767488\n",
      "43440640\n",
      "48767488\n",
      "43440640\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_properties(device).total_memory)\n",
    "print(torch.cuda.max_memory_allocated(device))\n",
    "print(torch.cuda.memory_allocated(device))\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.max_memory_allocated(device))\n",
    "print(torch.cuda.memory_allocated(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.812\n"
     ]
    }
   ],
   "source": [
    "netmodel.eval()\n",
    "\n",
    "netmodel.to(device)\n",
    "        \n",
    "accuracy = 0\n",
    "test_loss = 0\n",
    "\n",
    "for ii, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "    # flatten the imiga into a 784 element vector\n",
    "    images.resize_(images.size()[0], 784)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = netmodel.forward(images)\n",
    "\n",
    "    if nnDynamic == True:\n",
    "        ps = torch.exp(output)\n",
    "    else:\n",
    "        ps = F.softmax(output, dim=1)\n",
    "    \n",
    "\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out your network!\n",
    "netmodel.to(device)\n",
    "\n",
    "netmodel.eval()\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    output = netmodel.forward(img)\n",
    "\n",
    "if nnDynamic == True:\n",
    "    ps = torch.exp(output)\n",
    "else:\n",
    "    ps = F.softmax(output, dim=1)\n",
    "\n",
    "#print(ps.size())\n",
    "#print(ps.max(1)[1])\n",
    "\n",
    "#print(ps.device, img.device)\n",
    "if device != 'cpu':\n",
    "    ps = ps.cpu()\n",
    "    img = img.cpu()\n",
    "#print(ps.device, img.device)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
