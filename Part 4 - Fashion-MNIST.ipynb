{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packeges\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device. Device is either cuda:0 or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: F_MNIST_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: F_MNIST_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# define a trandform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data', train=True, download=True, transform=transform)\n",
    "#train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# download and load the testing data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data', train=False, download=True, transform=transform)\n",
    "#test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "shuffle_dataset = True\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "trainset_size = len(trainset)\n",
    "indices = list(range(trainset_size))\n",
    "split = int(np.floor(validation_split * trainset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "validation_loader = DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "helper.imshow(images[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fcl1): Linear(in_features=784, out_features=8000, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fcl2): Linear(in_features=8000, out_features=8000, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fcl3): Linear(in_features=8000, out_features=6000, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fcl4): Linear(in_features=6000, out_features=6000, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (fcl5): Linear(in_features=6000, out_features=4000, bias=True)\n",
       "  (relu5): ReLU()\n",
       "  (fcl6): Linear(in_features=4000, out_features=4000, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (fcl7): Linear(in_features=4000, out_features=200, bias=True)\n",
       "  (relu7): ReLU()\n",
       "  (fcl8): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu8): ReLU()\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = 784\n",
    "hidden_layers = [8000, 6000, 4000, 200]\n",
    "output_layer = 10\n",
    "\n",
    "netmodel = nn.Sequential(OrderedDict([\n",
    "    ('fcl1', nn.Linear(input_layer, hidden_layers[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fcl2', nn.Linear(hidden_layers[0], hidden_layers[0])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fcl3', nn.Linear(hidden_layers[0], hidden_layers[1])),\n",
    "    ('relu3', nn.ReLU()),\n",
    "    ('fcl4', nn.Linear(hidden_layers[1], hidden_layers[1])),\n",
    "    ('relu4', nn.ReLU()),\n",
    "    ('fcl5', nn.Linear(hidden_layers[1], hidden_layers[2])),\n",
    "    ('relu5', nn.ReLU()),\n",
    "    ('fcl6', nn.Linear(hidden_layers[2], hidden_layers[2])),\n",
    "    ('relu6', nn.ReLU()),\n",
    "    ('fcl7', nn.Linear(hidden_layers[2], hidden_layers[3])),\n",
    "    ('relu7', nn.ReLU()),\n",
    "    ('fcl8', nn.Linear(hidden_layers[3], hidden_layers[3])),\n",
    "    ('relu8', nn.ReLU()),\n",
    "    ('output', nn.Linear(hidden_layers[3], output_layer))\n",
    "]))\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(netmodel.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(netmodel.parameters(), lr=0.0003)\n",
    "#optimizer = optim.SGD(netmodel.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2...  Loss: 1.8158 Validation Loss: 1.564..  Validation Accuracy: 0.303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9d6fcede67e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 40\n",
    "\n",
    "netmodel.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        \n",
    "        # flatten the imiga into a 784 element vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = netmodel.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            netmodel.eval()\n",
    "        \n",
    "            accuracy = 0\n",
    "            valid_loss = 0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(validation_loader):\n",
    "                \n",
    "                \n",
    "                # flatten the imiga into a 784 element vector\n",
    "                images.resize_(images.size()[0], 784)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = netmodel.forward(images)\n",
    "                valid_loss += criterion(output, labels).item()\n",
    "                \n",
    "                ps = F.softmax(output, dim=1)\n",
    "                \n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                \n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Validation Loss: {:.3f}.. \".format(valid_loss/len(validation_loader)),\n",
    "                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(validation_loader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            netmodel.train()\n",
    "            \n",
    "print(\"Time for training and validation : {:.0f} minutes and {:.3f} seconds\".format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netmodel.eval()\n",
    "\n",
    "netmodel.to(device)\n",
    "        \n",
    "accuracy = 0\n",
    "test_loss = 0\n",
    "\n",
    "for ii, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "    # flatten the imiga into a 784 element vector\n",
    "    images.resize_(images.size()[0], 784)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = netmodel.forward(images)\n",
    "\n",
    "    ps = F.softmax(output, dim=1)\n",
    "\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your network!\n",
    "netmodel.to('cpu')\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = netmodel.forward(img)\n",
    "    \n",
    "ps = F.softmax(logits, dim=1)\n",
    "print(ps.size())\n",
    "print(ps.max(1)[1])\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
