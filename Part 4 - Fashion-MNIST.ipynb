{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packeges\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device. Device is either cuda:0 or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: F_MNIST_data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: F_MNIST_data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# define a trandform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data', train=True, download=True, transform=transform)\n",
    "#train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# download and load the testing data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data', train=False, download=True, transform=transform)\n",
    "#test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "shuffle_dataset = True\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "trainset_size = len(trainset)\n",
    "indices = list(range(trainset_size))\n",
    "split = int(np.floor(validation_split * trainset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "validation_loader = DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "helper.imshow(images[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fcl1): Linear(in_features=784, out_features=8000, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fcl2): Linear(in_features=8000, out_features=8000, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fcl3): Linear(in_features=8000, out_features=6000, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fcl4): Linear(in_features=6000, out_features=6000, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (fcl5): Linear(in_features=6000, out_features=4000, bias=True)\n",
       "  (relu5): ReLU()\n",
       "  (fcl6): Linear(in_features=4000, out_features=4000, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (fcl7): Linear(in_features=4000, out_features=200, bias=True)\n",
       "  (relu7): ReLU()\n",
       "  (fcl8): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu8): ReLU()\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = 784\n",
    "hidden_layers = [8000, 6000, 4000, 200]\n",
    "output_layer = 10\n",
    "\n",
    "netmodel = nn.Sequential(OrderedDict([\n",
    "    ('fcl1', nn.Linear(input_layer, hidden_layers[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fcl2', nn.Linear(hidden_layers[0], hidden_layers[0])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fcl3', nn.Linear(hidden_layers[0], hidden_layers[1])),\n",
    "    ('relu3', nn.ReLU()),\n",
    "    ('fcl4', nn.Linear(hidden_layers[1], hidden_layers[1])),\n",
    "    ('relu4', nn.ReLU()),\n",
    "    ('fcl5', nn.Linear(hidden_layers[1], hidden_layers[2])),\n",
    "    ('relu5', nn.ReLU()),\n",
    "    ('fcl6', nn.Linear(hidden_layers[2], hidden_layers[2])),\n",
    "    ('relu6', nn.ReLU()),\n",
    "    ('fcl7', nn.Linear(hidden_layers[2], hidden_layers[3])),\n",
    "    ('relu7', nn.ReLU()),\n",
    "    ('fcl8', nn.Linear(hidden_layers[3], hidden_layers[3])),\n",
    "    ('relu8', nn.ReLU()),\n",
    "    ('output', nn.Linear(hidden_layers[3], output_layer))\n",
    "]))\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(netmodel.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(netmodel.parameters(), lr=0.0003)\n",
    "#optimizer = optim.SGD(netmodel.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2...  Loss: 1.6871 Validation Loss: 1.231..  Validation Accuracy: 0.421\n",
      "Epoch: 1/2...  Loss: 1.0876 Validation Loss: 0.999..  Validation Accuracy: 0.586\n",
      "Epoch: 1/2...  Loss: 0.8899 Validation Loss: 0.892..  Validation Accuracy: 0.659\n",
      "Epoch: 1/2...  Loss: 0.8260 Validation Loss: 0.909..  Validation Accuracy: 0.630\n",
      "Epoch: 1/2...  Loss: 0.7827 Validation Loss: 0.729..  Validation Accuracy: 0.688\n",
      "Epoch: 1/2...  Loss: 0.7415 Validation Loss: 0.709..  Validation Accuracy: 0.706\n",
      "Epoch: 1/2...  Loss: 0.6750 Validation Loss: 0.631..  Validation Accuracy: 0.763\n",
      "Epoch: 1/2...  Loss: 0.6544 Validation Loss: 0.906..  Validation Accuracy: 0.679\n",
      "Epoch: 1/2...  Loss: 0.6934 Validation Loss: 0.634..  Validation Accuracy: 0.770\n",
      "Epoch: 1/2...  Loss: 0.6238 Validation Loss: 0.587..  Validation Accuracy: 0.777\n",
      "Epoch: 1/2...  Loss: 0.6043 Validation Loss: 0.622..  Validation Accuracy: 0.769\n",
      "Epoch: 1/2...  Loss: 0.6346 Validation Loss: 0.642..  Validation Accuracy: 0.769\n",
      "Epoch: 1/2...  Loss: 0.5998 Validation Loss: 0.581..  Validation Accuracy: 0.796\n",
      "Epoch: 1/2...  Loss: 0.6250 Validation Loss: 0.597..  Validation Accuracy: 0.775\n",
      "Epoch: 1/2...  Loss: 0.5861 Validation Loss: 0.692..  Validation Accuracy: 0.763\n",
      "Epoch: 1/2...  Loss: 0.6452 Validation Loss: 0.595..  Validation Accuracy: 0.780\n",
      "Epoch: 1/2...  Loss: 0.5630 Validation Loss: 0.571..  Validation Accuracy: 0.794\n",
      "Epoch: 1/2...  Loss: 0.5498 Validation Loss: 0.518..  Validation Accuracy: 0.799\n",
      "Epoch: 2/2...  Loss: 0.1273 Validation Loss: 0.555..  Validation Accuracy: 0.789\n",
      "Epoch: 2/2...  Loss: 0.5165 Validation Loss: 0.561..  Validation Accuracy: 0.798\n",
      "Epoch: 2/2...  Loss: 0.5408 Validation Loss: 0.541..  Validation Accuracy: 0.798\n",
      "Epoch: 2/2...  Loss: 0.5094 Validation Loss: 0.495..  Validation Accuracy: 0.820\n",
      "Epoch: 2/2...  Loss: 0.5133 Validation Loss: 0.562..  Validation Accuracy: 0.797\n",
      "Epoch: 2/2...  Loss: 0.5134 Validation Loss: 0.542..  Validation Accuracy: 0.808\n",
      "Epoch: 2/2...  Loss: 0.5516 Validation Loss: 0.553..  Validation Accuracy: 0.795\n",
      "Epoch: 2/2...  Loss: 0.4816 Validation Loss: 0.536..  Validation Accuracy: 0.798\n",
      "Epoch: 2/2...  Loss: 0.4871 Validation Loss: 0.481..  Validation Accuracy: 0.826\n",
      "Epoch: 2/2...  Loss: 0.5254 Validation Loss: 0.499..  Validation Accuracy: 0.814\n",
      "Epoch: 2/2...  Loss: 0.4690 Validation Loss: 0.490..  Validation Accuracy: 0.829\n",
      "Epoch: 2/2...  Loss: 0.4919 Validation Loss: 0.512..  Validation Accuracy: 0.808\n",
      "Epoch: 2/2...  Loss: 0.4891 Validation Loss: 0.500..  Validation Accuracy: 0.806\n",
      "Epoch: 2/2...  Loss: 0.4758 Validation Loss: 0.571..  Validation Accuracy: 0.785\n",
      "Epoch: 2/2...  Loss: 0.4936 Validation Loss: 0.574..  Validation Accuracy: 0.813\n",
      "Epoch: 2/2...  Loss: 0.5038 Validation Loss: 0.485..  Validation Accuracy: 0.836\n",
      "Epoch: 2/2...  Loss: 0.4878 Validation Loss: 0.468..  Validation Accuracy: 0.841\n",
      "Epoch: 2/2...  Loss: 0.4785 Validation Loss: 0.500..  Validation Accuracy: 0.817\n",
      "Epoch: 2/2...  Loss: 0.4424 Validation Loss: 0.456..  Validation Accuracy: 0.843\n",
      "Time for training and validation : 3 minutes and 34.480 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 40\n",
    "\n",
    "netmodel.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        \n",
    "        # flatten the imiga into a 784 element vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = netmodel.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            netmodel.eval()\n",
    "        \n",
    "            accuracy = 0\n",
    "            valid_loss = 0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(validation_loader):\n",
    "                \n",
    "                \n",
    "                # flatten the imiga into a 784 element vector\n",
    "                images.resize_(images.size()[0], 784)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = netmodel.forward(images)\n",
    "                valid_loss += criterion(output, labels).item()\n",
    "                \n",
    "                ps = F.softmax(output, dim=1)\n",
    "                \n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                \n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Validation Loss: {:.3f}.. \".format(valid_loss/len(validation_loader)),\n",
    "                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(validation_loader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            netmodel.train()\n",
    "            \n",
    "print(\"Time for training and validation : {:.0f} minutes and {:.3f} seconds\".format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "netmodel.eval()\n",
    "\n",
    "netmodel.to(device)\n",
    "        \n",
    "accuracy = 0\n",
    "test_loss = 0\n",
    "\n",
    "for ii, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "    # flatten the imiga into a 784 element vector\n",
    "    images.resize_(images.size()[0], 784)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = netmodel.forward(images)\n",
    "\n",
    "    ps = F.softmax(output, dim=1)\n",
    "\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([0])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out your network!\n",
    "netmodel.to('cpu')\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = netmodel.forward(img)\n",
    "    \n",
    "ps = F.softmax(logits, dim=1)\n",
    "print(ps.size())\n",
    "print(ps.max(1)[1])\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
