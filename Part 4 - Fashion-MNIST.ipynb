{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packeges\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device. Device is either cuda:0 or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: F_MNIST_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: F_MNIST_data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5,), std=(0.5,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# define a trandform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data', train=True, download=True, transform=transform)\n",
    "#train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# download and load the testing data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data', train=False, download=True, transform=transform)\n",
    "#test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "shuffle_dataset = True\n",
    "validation_split = .2\n",
    "random_seed= 42\n",
    "\n",
    "trainset_size = len(trainset)\n",
    "indices = list(range(trainset_size))\n",
    "split = int(np.floor(validation_split * trainset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(len(train_indices))\n",
    "print(len(val_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "validation_loader = DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "helper.imshow(images[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fcl1): Linear(in_features=784, out_features=800, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fcl2): Linear(in_features=800, out_features=800, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fcl3): Linear(in_features=800, out_features=600, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fcl4): Linear(in_features=600, out_features=600, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (fcl5): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (relu5): ReLU()\n",
       "  (fcl6): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (fcl7): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (relu7): ReLU()\n",
       "  (fcl8): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu8): ReLU()\n",
       "  (output): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = 784\n",
    "hidden_layers = [800, 600, 400, 200]\n",
    "output_layer = 10\n",
    "\n",
    "netmodel = nn.Sequential(OrderedDict([\n",
    "    ('fcl1', nn.Linear(input_layer, hidden_layers[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fcl2', nn.Linear(hidden_layers[0], hidden_layers[0])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fcl3', nn.Linear(hidden_layers[0], hidden_layers[1])),\n",
    "    ('relu3', nn.ReLU()),\n",
    "    ('fcl4', nn.Linear(hidden_layers[1], hidden_layers[1])),\n",
    "    ('relu4', nn.ReLU()),\n",
    "    ('fcl5', nn.Linear(hidden_layers[1], hidden_layers[2])),\n",
    "    ('relu5', nn.ReLU()),\n",
    "    ('fcl6', nn.Linear(hidden_layers[2], hidden_layers[2])),\n",
    "    ('relu6', nn.ReLU()),\n",
    "    ('fcl7', nn.Linear(hidden_layers[2], hidden_layers[3])),\n",
    "    ('relu7', nn.ReLU()),\n",
    "    ('fcl8', nn.Linear(hidden_layers[3], hidden_layers[3])),\n",
    "    ('relu8', nn.ReLU()),\n",
    "    ('output', nn.Linear(hidden_layers[3], output_layer))\n",
    "]))\n",
    "\n",
    "netmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(netmodel.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(netmodel.parameters(), lr=0.0003)\n",
    "#optimizer = optim.SGD(netmodel.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30...  Loss: 0.0689 Validation Loss: 0.517..  Validation Accuracy: 0.893\n",
      "Epoch: 1/30...  Loss: 0.0731 Validation Loss: 0.543..  Validation Accuracy: 0.892\n",
      "Epoch: 1/30...  Loss: 0.0557 Validation Loss: 0.576..  Validation Accuracy: 0.895\n",
      "Epoch: 1/30...  Loss: 0.0781 Validation Loss: 0.516..  Validation Accuracy: 0.890\n",
      "Epoch: 1/30...  Loss: 0.0702 Validation Loss: 0.570..  Validation Accuracy: 0.881\n",
      "Epoch: 1/30...  Loss: 0.0906 Validation Loss: 0.561..  Validation Accuracy: 0.889\n",
      "Epoch: 1/30...  Loss: 0.0895 Validation Loss: 0.521..  Validation Accuracy: 0.891\n",
      "Epoch: 1/30...  Loss: 0.0954 Validation Loss: 0.548..  Validation Accuracy: 0.883\n",
      "Epoch: 1/30...  Loss: 0.0907 Validation Loss: 0.514..  Validation Accuracy: 0.889\n",
      "Epoch: 1/30...  Loss: 0.0817 Validation Loss: 0.547..  Validation Accuracy: 0.887\n",
      "Epoch: 1/30...  Loss: 0.1245 Validation Loss: 0.440..  Validation Accuracy: 0.892\n",
      "Epoch: 1/30...  Loss: 0.0884 Validation Loss: 0.453..  Validation Accuracy: 0.897\n",
      "Epoch: 1/30...  Loss: 0.0821 Validation Loss: 0.488..  Validation Accuracy: 0.892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-c2b2086c40c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 40\n",
    "\n",
    "netmodel.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        \n",
    "        # flatten the imiga into a 784 element vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = netmodel.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            netmodel.eval()\n",
    "        \n",
    "            accuracy = 0\n",
    "            valid_loss = 0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(validation_loader):\n",
    "                \n",
    "                \n",
    "                # flatten the imiga into a 784 element vector\n",
    "                images.resize_(images.size()[0], 784)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = netmodel.forward(images)\n",
    "                valid_loss += criterion(output, labels).item()\n",
    "                \n",
    "                ps = F.softmax(output, dim=1)\n",
    "                \n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                \n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Validation Loss: {:.3f}.. \".format(valid_loss/len(validation_loader)),\n",
    "                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(validation_loader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            netmodel.train()\n",
    "            \n",
    "print(\"Time for training and validation : {:.0f} minutes and {:.3f} seconds\".format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.887\n"
     ]
    }
   ],
   "source": [
    "netmodel.eval()\n",
    "\n",
    "netmodel.to(device)\n",
    "        \n",
    "accuracy = 0\n",
    "test_loss = 0\n",
    "\n",
    "for ii, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "\n",
    "    # flatten the imiga into a 784 element vector\n",
    "    images.resize_(images.size()[0], 784)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = netmodel.forward(images)\n",
    "\n",
    "    ps = F.softmax(output, dim=1)\n",
    "\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([6])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out your network!\n",
    "netmodel.to('cpu')\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = netmodel.forward(img)\n",
    "    \n",
    "ps = F.softmax(logits, dim=1)\n",
    "print(ps.size())\n",
    "print(ps.max(1)[1])\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
